{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of AutoEncoders.ipynb","provenance":[{"file_id":"1KYKvdVQ_r32eT7o-_QqOcD_XtCc92F1t","timestamp":1609842870605},{"file_id":"1TBJsYNflgFHbS2M53gZngpTVKtvnLSDc","timestamp":1591633331181}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"K4f4JG1gdKqj"},"source":["#AutoEncoders"]},{"cell_type":"markdown","metadata":{"id":"1jbiqOK7dLGG"},"source":["##Downloading the dataset"]},{"cell_type":"markdown","metadata":{"id":"EOBJ8UCXdY0g"},"source":["##Importing the libraries"]},{"cell_type":"code","metadata":{"id":"_LvGeU1CeCtg","executionInfo":{"status":"ok","timestamp":1609850965863,"user_tz":-240,"elapsed":1406,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.autograd import Variable"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pM04FyMudkoK"},"source":["## Importing the dataset\n"]},{"cell_type":"code","metadata":{"id":"UJw2p3-Cewo4","executionInfo":{"status":"ok","timestamp":1609850970012,"user_tz":-240,"elapsed":5545,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["# We won't be using this dataset.\n","movies = pd.read_csv('movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n","users = pd.read_csv('users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n","ratings = pd.read_csv('ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTIbE2tkdkwP"},"source":["## Preparing the training set and the test set\n"]},{"cell_type":"code","metadata":{"id":"2usLKJBEgPE2","executionInfo":{"status":"ok","timestamp":1609850970013,"user_tz":-240,"elapsed":5541,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["training_set = pd.read_csv('u1.base', delimiter = '\\t') \n","training_set = np.array(training_set, dtype = 'int') # Transforming the dataframe to np array\n","test_set = pd.read_csv('u1.test', delimiter = '\\t')\n","test_set = np.array(test_set, dtype = 'int') # Transforming the dataframe to np array"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCf8HjSydk4s"},"source":["## Getting the number of users and movies\n"]},{"cell_type":"code","metadata":{"id":"gPaGZqdniC5m","executionInfo":{"status":"ok","timestamp":1609850970014,"user_tz":-240,"elapsed":5535,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0]))) # The number of users that we have in the test set and the training set which is the maximum user ID from the training set\n","# and the test set \n","\n","nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1]))) # The number of users that we have in the test set and the training set which is the maximum user ID \n","# from the training set and the test set "],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-w4-hVidlAm"},"source":["## Converting the data into an array with users in lines and movies in columns\n"]},{"cell_type":"code","metadata":{"id":"-wASs2YFiDaa","executionInfo":{"status":"ok","timestamp":1609850970015,"user_tz":-240,"elapsed":5530,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["def convert(data):\n","  new_data = []\n","\n","  for id_users in range(1, nb_users+1):\n","    id_movies = data[:, 1] [data[:, 0] == id_users] # This a syntax of a conditional statment in pyhton\n","    \n","    id_ratings = data[:, 2] [data[:, 0] == id_users] # This a syntax of a conditional statment in pyhton\n","\n","    ratings = np.zeros(nb_movies) # Here we are initializing a new list of zeros with the size of all the movies that we have so we can replace the zeros\n","    # with the rating if it is there\n","\n","    ratings[id_movies - 1] = id_ratings # Here we are trying to build the ratings array by finding the matches for it in the id_ratings so if there is a\n","    # raing for the movie id mentioned then the ratings will be transfered or equaled otherwise it will still be 0\n","\n","    new_data.append(list(ratings)) # appending the ratings of all the movies in the users list that will result in a 2d list\n","  \n","  return new_data\n","\n","training_set = convert(training_set)\n","test_set = convert(test_set)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMmhuUpldlHo"},"source":["## Converting the data into Torch tensors\n"]},{"cell_type":"code","metadata":{"id":"TwD-KD8yiEEw","executionInfo":{"status":"ok","timestamp":1609850970722,"user_tz":-240,"elapsed":6230,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["#Remember the tesnor is a multi-dimensional matrix with one type in it \n","\n","training_set = torch.FloatTensor(training_set) # We are converting the 2d list of training set into a 2d Floattensor so we can build the architecture of the boltzmann network\n","# so we can do the training and the building much effecient with tensors\n","\n","test_set = torch.FloatTensor(test_set) # We are converting the 2d list of test set into a 2d Floattensor so we can build the architecture of the boltzmann network\n","# so we can do the training and the building much effecient with tensors"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_qPZka9KuAW","executionInfo":{"status":"ok","timestamp":1609850970723,"user_tz":-240,"elapsed":6225,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}},"outputId":"469a3cc3-936a-4248-9ce0-2fcb06ca0f78"},"source":["print(training_set)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n","        [4., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [5., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 5., 0.,  ..., 0., 0., 0.]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKARrwpMKuIE","executionInfo":{"status":"ok","timestamp":1609850970724,"user_tz":-240,"elapsed":6218,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}},"outputId":"623f51e9-95ad-4f4c-97bb-2d9af2e27245"},"source":["print(test_set)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6kkL8NkkdlZj"},"source":["## Creating the architecture of the Neural Network\n"]},{"cell_type":"code","metadata":{"id":"U1V0Q2HpMkAl","executionInfo":{"status":"ok","timestamp":1609850970725,"user_tz":-240,"elapsed":6212,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["class SAE(nn.Module):\r\n","  def __init__(self):\r\n","    super(SAE, self).__init__() # Here we are making sure that we get all the methods and attributes of the class nn.Module\r\n","    self.full_connection_1 = nn.Linear(nb_movies, 50) # Here we are making the the first full connection leyer which is the input \r\n","    # layer with dim (nb_movies) for the inputs and outputs to 20 hidden neurons, which are the features extracted from the\r\n","    # input layer which decreases the amount of features used depending on features the movies is based on (genre, actors, awards and etc)\r\n","    \r\n","    self.full_connection_2 = nn.Linear(50, 10) # Here we are making the the second full connection leyer which is the first \r\n","    # hidden layer with dim (20) for the inputs and outputs to 10 hidden neurons, which are the features extracted from the\r\n","    # previous layer which decreases the amount of features used depending on features the movies is based on (genre, actors, awards and etc)\r\n","\r\n","    self.full_connection_3 = nn.Linear(10, 50) # Here we are making the the third full connection leyer which is the second \r\n","    # hidden layer with dim (10) for the inputs and outputs to 20 hidden neurons, which are the features extracted from the\r\n","    # previous layer and increases the amount of features used becuase now we are entering the decoding phase\r\n","\r\n","    self.full_connection_4 = nn.Linear(50, nb_movies) # Here we are making the the fourth full connection leyer which is the output \r\n","    # layer with dim (20) for the inputs and outputs to (nb_movies) output neurons, which are the features extracted from the\r\n","    # previous layer and increases the amount of features used becuase now we are entering the decoding phase and outputing the \r\n","    # xecat amount number of features entered which is the number of movies we have\r\n","    \r\n","    self.activation = nn.Sigmoid() # Here we are making an attribute that is the essence of training the Auto Encoder\r\n","\r\n","  \r\n","  def forward(self, x):\r\n","    x = self.activation(self.full_connection_1(x)) # Here we are appltying the sigmoid activation function on the input matrix of features\r\n","    # To encode it using the input layer\r\n","\r\n","    x = self.activation(self.full_connection_2(x)) # Here we are appltying the sigmoid activation function on the input matrix of features from\r\n","    # the input layer To edncode it using the first hidden layer\r\n","    \r\n","    x = self.activation(self.full_connection_3(x)) # Here we are appltying the sigmoid activation function on the input matrix of features from\r\n","    # the first hidden layer To edncode it using the second hidden layer\r\n","\r\n","    x = self.full_connection_4(x) # Here we are in the output layer where the final decoding layer and procedure happens ,and where the number of \r\n","    # features go back to their original number which is nb_movies\r\n","\r\n","    return x #returning x so we don't modify the original matrix of feature x\r\n","\r\n","sae = SAE()\r\n","criterion = nn.MSELoss()\r\n","optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.6)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gy59alAdloL"},"source":["## Training the SAE\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QNzk5BuVGgt","executionInfo":{"status":"ok","timestamp":1609851325081,"user_tz":-240,"elapsed":360563,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}},"outputId":"bb5c40ae-be49-4004-c723-ca09c8f1acc8"},"source":["nb_epoch = 200\r\n","\r\n","for epoch in range(1, nb_epoch+1):\r\n","  train_loss = 0\r\n","  counter = 0.\r\n","  for id_user in range(nb_users):\r\n","    inputs = Variable(training_set[id_user]).unsqueeze(0)\r\n","    target = inputs.clone()\r\n","    if torch.sum(target.data > 0) > 0:\r\n","      output = sae(inputs)\r\n","      target.require_grad = False\r\n","      output[ target==0 ] = 0\r\n","      loss = criterion(output, target)\r\n","      mean_corrector = nb_movies/float(torch.sum(target.data > 0)+ 1e-10)\r\n","      loss.backward() # Decides the drirection of the updated weights\r\n","      train_loss += np.sqrt(loss.data * mean_corrector)\r\n","      counter += 1.\r\n","      optimizer.step() # Decides the intensity of which the weights gets updated\r\n","  \r\n","  print('epoch: '+str(epoch)+\" loss: \"+str(train_loss/counter))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["epoch: 1 loss: tensor(1.5851)\n","epoch: 2 loss: tensor(1.0831)\n","epoch: 3 loss: tensor(1.0652)\n","epoch: 4 loss: tensor(1.0582)\n","epoch: 5 loss: tensor(1.0545)\n","epoch: 6 loss: tensor(1.0527)\n","epoch: 7 loss: tensor(1.0499)\n","epoch: 8 loss: tensor(1.0488)\n","epoch: 9 loss: tensor(1.0453)\n","epoch: 10 loss: tensor(1.0431)\n","epoch: 11 loss: tensor(1.0433)\n","epoch: 12 loss: tensor(1.0378)\n","epoch: 13 loss: tensor(1.0395)\n","epoch: 14 loss: tensor(1.0333)\n","epoch: 15 loss: tensor(1.0356)\n","epoch: 16 loss: tensor(1.0299)\n","epoch: 17 loss: tensor(1.0321)\n","epoch: 18 loss: tensor(1.0255)\n","epoch: 19 loss: tensor(1.0278)\n","epoch: 20 loss: tensor(1.0228)\n","epoch: 21 loss: tensor(1.0214)\n","epoch: 22 loss: tensor(1.0181)\n","epoch: 23 loss: tensor(1.0164)\n","epoch: 24 loss: tensor(1.0158)\n","epoch: 25 loss: tensor(1.0142)\n","epoch: 26 loss: tensor(1.0123)\n","epoch: 27 loss: tensor(1.0096)\n","epoch: 28 loss: tensor(1.0093)\n","epoch: 29 loss: tensor(1.0081)\n","epoch: 30 loss: tensor(1.0072)\n","epoch: 31 loss: tensor(1.0078)\n","epoch: 32 loss: tensor(1.0037)\n","epoch: 33 loss: tensor(1.0039)\n","epoch: 34 loss: tensor(0.9985)\n","epoch: 35 loss: tensor(1.0007)\n","epoch: 36 loss: tensor(0.9937)\n","epoch: 37 loss: tensor(0.9915)\n","epoch: 38 loss: tensor(0.9860)\n","epoch: 39 loss: tensor(0.9858)\n","epoch: 40 loss: tensor(0.9833)\n","epoch: 41 loss: tensor(0.9833)\n","epoch: 42 loss: tensor(0.9852)\n","epoch: 43 loss: tensor(0.9857)\n","epoch: 44 loss: tensor(0.9805)\n","epoch: 45 loss: tensor(0.9800)\n","epoch: 46 loss: tensor(0.9783)\n","epoch: 47 loss: tensor(0.9742)\n","epoch: 48 loss: tensor(0.9728)\n","epoch: 49 loss: tensor(0.9726)\n","epoch: 50 loss: tensor(0.9719)\n","epoch: 51 loss: tensor(0.9685)\n","epoch: 52 loss: tensor(0.9655)\n","epoch: 53 loss: tensor(0.9655)\n","epoch: 54 loss: tensor(0.9639)\n","epoch: 55 loss: tensor(0.9627)\n","epoch: 56 loss: tensor(0.9644)\n","epoch: 57 loss: tensor(0.9614)\n","epoch: 58 loss: tensor(0.9614)\n","epoch: 59 loss: tensor(0.9598)\n","epoch: 60 loss: tensor(0.9604)\n","epoch: 61 loss: tensor(0.9574)\n","epoch: 62 loss: tensor(0.9577)\n","epoch: 63 loss: tensor(0.9555)\n","epoch: 64 loss: tensor(0.9556)\n","epoch: 65 loss: tensor(0.9539)\n","epoch: 66 loss: tensor(0.9531)\n","epoch: 67 loss: tensor(0.9530)\n","epoch: 68 loss: tensor(0.9515)\n","epoch: 69 loss: tensor(0.9511)\n","epoch: 70 loss: tensor(0.9501)\n","epoch: 71 loss: tensor(0.9493)\n","epoch: 72 loss: tensor(0.9493)\n","epoch: 73 loss: tensor(0.9485)\n","epoch: 74 loss: tensor(0.9493)\n","epoch: 75 loss: tensor(0.9478)\n","epoch: 76 loss: tensor(0.9480)\n","epoch: 77 loss: tensor(0.9487)\n","epoch: 78 loss: tensor(0.9461)\n","epoch: 79 loss: tensor(0.9453)\n","epoch: 80 loss: tensor(0.9458)\n","epoch: 81 loss: tensor(0.9449)\n","epoch: 82 loss: tensor(0.9445)\n","epoch: 83 loss: tensor(0.9437)\n","epoch: 84 loss: tensor(0.9439)\n","epoch: 85 loss: tensor(0.9427)\n","epoch: 86 loss: tensor(0.9437)\n","epoch: 87 loss: tensor(0.9424)\n","epoch: 88 loss: tensor(0.9427)\n","epoch: 89 loss: tensor(0.9415)\n","epoch: 90 loss: tensor(0.9415)\n","epoch: 91 loss: tensor(0.9406)\n","epoch: 92 loss: tensor(0.9408)\n","epoch: 93 loss: tensor(0.9399)\n","epoch: 94 loss: tensor(0.9399)\n","epoch: 95 loss: tensor(0.9391)\n","epoch: 96 loss: tensor(0.9393)\n","epoch: 97 loss: tensor(0.9384)\n","epoch: 98 loss: tensor(0.9387)\n","epoch: 99 loss: tensor(0.9377)\n","epoch: 100 loss: tensor(0.9379)\n","epoch: 101 loss: tensor(0.9370)\n","epoch: 102 loss: tensor(0.9373)\n","epoch: 103 loss: tensor(0.9362)\n","epoch: 104 loss: tensor(0.9366)\n","epoch: 105 loss: tensor(0.9352)\n","epoch: 106 loss: tensor(0.9358)\n","epoch: 107 loss: tensor(0.9344)\n","epoch: 108 loss: tensor(0.9350)\n","epoch: 109 loss: tensor(0.9338)\n","epoch: 110 loss: tensor(0.9343)\n","epoch: 111 loss: tensor(0.9332)\n","epoch: 112 loss: tensor(0.9339)\n","epoch: 113 loss: tensor(0.9325)\n","epoch: 114 loss: tensor(0.9331)\n","epoch: 115 loss: tensor(0.9316)\n","epoch: 116 loss: tensor(0.9325)\n","epoch: 117 loss: tensor(0.9310)\n","epoch: 118 loss: tensor(0.9320)\n","epoch: 119 loss: tensor(0.9305)\n","epoch: 120 loss: tensor(0.9312)\n","epoch: 121 loss: tensor(0.9298)\n","epoch: 122 loss: tensor(0.9305)\n","epoch: 123 loss: tensor(0.9294)\n","epoch: 124 loss: tensor(0.9298)\n","epoch: 125 loss: tensor(0.9289)\n","epoch: 126 loss: tensor(0.9294)\n","epoch: 127 loss: tensor(0.9280)\n","epoch: 128 loss: tensor(0.9289)\n","epoch: 129 loss: tensor(0.9275)\n","epoch: 130 loss: tensor(0.9284)\n","epoch: 131 loss: tensor(0.9268)\n","epoch: 132 loss: tensor(0.9276)\n","epoch: 133 loss: tensor(0.9262)\n","epoch: 134 loss: tensor(0.9269)\n","epoch: 135 loss: tensor(0.9259)\n","epoch: 136 loss: tensor(0.9265)\n","epoch: 137 loss: tensor(0.9254)\n","epoch: 138 loss: tensor(0.9258)\n","epoch: 139 loss: tensor(0.9247)\n","epoch: 140 loss: tensor(0.9257)\n","epoch: 141 loss: tensor(0.9243)\n","epoch: 142 loss: tensor(0.9252)\n","epoch: 143 loss: tensor(0.9242)\n","epoch: 144 loss: tensor(0.9248)\n","epoch: 145 loss: tensor(0.9234)\n","epoch: 146 loss: tensor(0.9243)\n","epoch: 147 loss: tensor(0.9228)\n","epoch: 148 loss: tensor(0.9236)\n","epoch: 149 loss: tensor(0.9227)\n","epoch: 150 loss: tensor(0.9228)\n","epoch: 151 loss: tensor(0.9221)\n","epoch: 152 loss: tensor(0.9227)\n","epoch: 153 loss: tensor(0.9218)\n","epoch: 154 loss: tensor(0.9222)\n","epoch: 155 loss: tensor(0.9213)\n","epoch: 156 loss: tensor(0.9219)\n","epoch: 157 loss: tensor(0.9207)\n","epoch: 158 loss: tensor(0.9214)\n","epoch: 159 loss: tensor(0.9205)\n","epoch: 160 loss: tensor(0.9210)\n","epoch: 161 loss: tensor(0.9199)\n","epoch: 162 loss: tensor(0.9205)\n","epoch: 163 loss: tensor(0.9195)\n","epoch: 164 loss: tensor(0.9205)\n","epoch: 165 loss: tensor(0.9191)\n","epoch: 166 loss: tensor(0.9197)\n","epoch: 167 loss: tensor(0.9189)\n","epoch: 168 loss: tensor(0.9192)\n","epoch: 169 loss: tensor(0.9182)\n","epoch: 170 loss: tensor(0.9188)\n","epoch: 171 loss: tensor(0.9178)\n","epoch: 172 loss: tensor(0.9183)\n","epoch: 173 loss: tensor(0.9174)\n","epoch: 174 loss: tensor(0.9176)\n","epoch: 175 loss: tensor(0.9169)\n","epoch: 176 loss: tensor(0.9170)\n","epoch: 177 loss: tensor(0.9161)\n","epoch: 178 loss: tensor(0.9162)\n","epoch: 179 loss: tensor(0.9155)\n","epoch: 180 loss: tensor(0.9156)\n","epoch: 181 loss: tensor(0.9148)\n","epoch: 182 loss: tensor(0.9148)\n","epoch: 183 loss: tensor(0.9135)\n","epoch: 184 loss: tensor(0.9135)\n","epoch: 185 loss: tensor(0.9122)\n","epoch: 186 loss: tensor(0.9125)\n","epoch: 187 loss: tensor(0.9110)\n","epoch: 188 loss: tensor(0.9109)\n","epoch: 189 loss: tensor(0.9092)\n","epoch: 190 loss: tensor(0.9097)\n","epoch: 191 loss: tensor(0.9081)\n","epoch: 192 loss: tensor(0.9083)\n","epoch: 193 loss: tensor(0.9064)\n","epoch: 194 loss: tensor(0.9071)\n","epoch: 195 loss: tensor(0.9053)\n","epoch: 196 loss: tensor(0.9056)\n","epoch: 197 loss: tensor(0.9037)\n","epoch: 198 loss: tensor(0.9044)\n","epoch: 199 loss: tensor(0.9029)\n","epoch: 200 loss: tensor(0.9031)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bak5uc8gd-gX"},"source":["## Testing the SAE\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75Xbj4kaehP2","executionInfo":{"status":"ok","timestamp":1609851325082,"user_tz":-240,"elapsed":360557,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}},"outputId":"e8b3cc22-a47b-45c4-8948-53d61ac86d81"},"source":["test_loss = 0\r\n","counter = 0.\r\n","for id_user in range(nb_users):\r\n","  inputs = Variable(training_set[id_user]).unsqueeze(0)\r\n","  target = Variable(test_set[id_user]).unsqueeze(0)\r\n","  if torch.sum(target.data > 0) > 0:\r\n","    output = sae(inputs)\r\n","    target.require_grad = False\r\n","    output[ target==0 ] = 0\r\n","    loss = criterion(output, target)\r\n","    mean_corrector = nb_movies/float(torch.sum(target.data > 0)+ 1e-10)\r\n","    test_loss += np.sqrt(loss.data * mean_corrector)\r\n","    counter += 1.\r\n","\r\n","print(\"test loss: \"+str(test_loss/counter))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["test loss: tensor(0.9432)\n"],"name":"stdout"}]}]}