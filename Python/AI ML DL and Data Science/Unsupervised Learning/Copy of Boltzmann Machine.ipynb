{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Boltzmann Machine.ipynb","provenance":[{"file_id":"1JxIEe_TAcz-utfLKTqjbo3foOtqDGD0s","timestamp":1609771568643}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K4f4JG1gdKqj"},"source":["#Boltzmann Machine"]},{"cell_type":"markdown","metadata":{"id":"1jbiqOK7dLGG"},"source":["##Downloading the dataset"]},{"cell_type":"markdown","metadata":{"id":"XL5MEkLcfRD2"},"source":["###ML-100K"]},{"cell_type":"code","metadata":{"id":"rjOPzue7FCXJ","colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"status":"ok","timestamp":1591629496532,"user_tz":-330,"elapsed":17382,"user":{"displayName":"Nirav Agarwal (RA1811003010570)","photoUrl":"","userId":"01096445040044781974"}},"outputId":"44d3a628-f522-4d0d-efdf-009b7d3a28df"},"source":["!wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n","!unzip ml-100k.zip\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-08 15:18:04--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n","Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n","Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4924029 (4.7M) [application/zip]\n","Saving to: ‘ml-100k.zip’\n","\n","\rml-100k.zip           0%[                    ]       0  --.-KB/s               \rml-100k.zip          18%[==>                 ] 884.39K  3.96MB/s               \rml-100k.zip         100%[===================>]   4.70M  15.6MB/s    in 0.3s    \n","\n","2020-06-08 15:18:04 (15.6 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n","\n","Archive:  ml-100k.zip\n","   creating: ml-100k/\n","  inflating: ml-100k/allbut.pl       \n","  inflating: ml-100k/mku.sh          \n","  inflating: ml-100k/README          \n","  inflating: ml-100k/u.data          \n","  inflating: ml-100k/u.genre         \n","  inflating: ml-100k/u.info          \n","  inflating: ml-100k/u.item          \n","  inflating: ml-100k/u.occupation    \n","  inflating: ml-100k/u.user          \n","  inflating: ml-100k/u1.base         \n","  inflating: ml-100k/u1.test         \n","  inflating: ml-100k/u2.base         \n","  inflating: ml-100k/u2.test         \n","  inflating: ml-100k/u3.base         \n","  inflating: ml-100k/u3.test         \n","  inflating: ml-100k/u4.base         \n","  inflating: ml-100k/u4.test         \n","  inflating: ml-100k/u5.base         \n","  inflating: ml-100k/u5.test         \n","  inflating: ml-100k/ua.base         \n","  inflating: ml-100k/ua.test         \n","  inflating: ml-100k/ub.base         \n","  inflating: ml-100k/ub.test         \n","ml-100k  ml-100k.zip  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9Xis6ldDfTs6"},"source":["###ML-1M"]},{"cell_type":"code","metadata":{"id":"LOly1yfAfTjd","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"ok","timestamp":1591629505410,"user_tz":-330,"elapsed":26251,"user":{"displayName":"Nirav Agarwal (RA1811003010570)","photoUrl":"","userId":"01096445040044781974"}},"outputId":"22029b8c-79f2-46a2-a745-cdce83582b40"},"source":["!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n","!unzip ml-1m.zip\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-08 15:18:16--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\n","Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n","Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5917549 (5.6M) [application/zip]\n","Saving to: ‘ml-1m.zip’\n","\n","ml-1m.zip           100%[===================>]   5.64M  2.44MB/s    in 2.3s    \n","\n","2020-06-08 15:18:19 (2.44 MB/s) - ‘ml-1m.zip’ saved [5917549/5917549]\n","\n","Archive:  ml-1m.zip\n","   creating: ml-1m/\n","  inflating: ml-1m/movies.dat        \n","  inflating: ml-1m/ratings.dat       \n","  inflating: ml-1m/README            \n","  inflating: ml-1m/users.dat         \n","ml-100k  ml-100k.zip  ml-1m  ml-1m.zip\tsample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EOBJ8UCXdY0g"},"source":["##Importing the libraries"]},{"cell_type":"code","metadata":{"id":"Qm56vBW03Urx","executionInfo":{"status":"ok","timestamp":1609791375535,"user_tz":-240,"elapsed":993,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["import numpy as np\r\n","import pandas as pd\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.parallel\r\n","import torch.optim as optim\r\n","import torch.utils.data\r\n","from torch.autograd import Variable"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pM04FyMudkoK"},"source":["## Importing the dataset\n"]},{"cell_type":"code","metadata":{"id":"7UvQsZ1-3rag","executionInfo":{"status":"ok","timestamp":1609791380581,"user_tz":-240,"elapsed":5214,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["movies = pd.read_csv('movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\r\n","users =  pd.read_csv('users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\r\n","ratings =  pd.read_csv('ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTIbE2tkdkwP"},"source":["## Preparing the training set and the test set\n"]},{"cell_type":"code","metadata":{"id":"bCcKVE5X5PDg","executionInfo":{"status":"ok","timestamp":1609791380584,"user_tz":-240,"elapsed":3827,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["training_set = pd.read_csv('u1.base', delimiter = '\\t') \r\n","training_set = np.array(training_set, dtype = 'int') # Transforming the dataframe to np array\r\n","test_set = pd.read_csv('u1.test', delimiter = '\\t')\r\n","test_set = np.array(test_set, dtype = 'int') # Transforming the dataframe to np array"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCf8HjSydk4s"},"source":["## Getting the number of users and movies\n"]},{"cell_type":"code","metadata":{"id":"m9ZOdImTCff7","executionInfo":{"status":"ok","timestamp":1609791380585,"user_tz":-240,"elapsed":2458,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0]))) # The number of users that we have in the test set and the training set which is the maximum user ID from the training set\r\n","# and the test set \r\n","\r\n","nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1]))) # The number of users that we have in the test set and the training set which is the maximum user ID \r\n","# from the training set and the test set "],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-w4-hVidlAm"},"source":["## Converting the data into an array with users in lines and movies in columns\n"]},{"cell_type":"code","metadata":{"id":"aUSUuEAaEulu","executionInfo":{"status":"ok","timestamp":1609791381083,"user_tz":-240,"elapsed":1869,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["def convert(data):\r\n","  new_data = []\r\n","\r\n","  for id_users in range(1, nb_users+1):\r\n","    id_movies = data[:, 1] [data[:, 0] == id_users] # This a syntax of a conditional statment in pyhton\r\n","    \r\n","    id_ratings = data[:, 2] [data[:, 0] == id_users] # This a syntax of a conditional statment in pyhton\r\n","\r\n","    ratings = np.zeros(nb_movies) # Here we are initializing a new lits of zeros with the size of all the movies that we have so we can replace the zeros\r\n","    # with the rating if it is there\r\n","\r\n","    ratings[id_movies - 1] = id_ratings # Here we are trying to build the ratings array by finding the matches for it in the id_ratings so if there is a\r\n","    # raing for the movie id mentioned then the ratings will be transfered or equaled otherwise it will still be 0\r\n","\r\n","    new_data.append(list(ratings)) # appending the ratings of all the movies in the users list that will result in a 2d list\r\n","  \r\n","  return new_data\r\n","\r\n","training_set = convert(training_set)\r\n","test_set = convert(test_set)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMmhuUpldlHo"},"source":["## Converting the data into Torch tensors\n"]},{"cell_type":"code","metadata":{"id":"jA9mijy0Lma-","executionInfo":{"status":"ok","timestamp":1609791381678,"user_tz":-240,"elapsed":1076,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["#Remember the tesnor is a multi-dimensional matrix with one type in it \r\n","\r\n","training_set = torch.FloatTensor(training_set) # We are converting the 2d list of training set into a 2d Floattensor so we can build the architecture of the boltzmann network\r\n","# so we can do the training and the building much effecient with tensors\r\n","\r\n","test_set = torch.FloatTensor(test_set) # We are converting the 2d list of test set into a 2d Floattensor so we can build the architecture of the boltzmann network\r\n","# so we can do the training and the building much effecient with tensors"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIPruubGdlPW"},"source":["## Converting the ratings into binary ratings 1 (Liked) or 0 (Not Liked)\n"]},{"cell_type":"code","metadata":{"id":"itenoUdSOSWx","executionInfo":{"status":"ok","timestamp":1609791383004,"user_tz":-240,"elapsed":635,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["training_set[training_set == 0] = -1# we want to change all the ratings to either 0 or 1 so we have to give another category for the missing data whcih is -1\r\n","training_set[training_set == 1] = 0 # we want to change all the ratings to either 0 or 1 so if the user put a rating 1 for the movie that indicates that the user\r\n","# did not like the movie\r\n","training_set[training_set == 2] = 0 # we want to change all the ratings to either 0 or 1 so if the user put a rating 2 for the movie that indicates that the user\r\n","# did not like the movie we couldn't do the previous this step and the previous step at once becuase FloatTensors do not accept or statements\r\n","\r\n","training_set[training_set >= 3] = 1 # we want to change all the ratings to either 0 or 1 so if the user put a rating 3 or higher for the movie that indicates \r\n","# that the user did not like the movie\r\n","\r\n","test_set[test_set == 0] = -1# we want to change all the ratings to either 0 or 1 so we have to give another category for the missing data whcih is -1\r\n","test_set[test_set == 1] = 0 # we want to change all the ratings to either 0 or 1 so if the user put a rating 1 for the movie that indicates that the user\r\n","# did not like the movie\r\n","test_set[test_set == 2] = 0 # we want to change all the ratings to either 0 or 1 so if the user put a rating 2 for the movie that indicates that the user\r\n","# did not like the movie we couldn't do the previous this step and the previous step at once becuase FloatTensors do not accept or statements\r\n","\r\n","test_set[test_set >= 3] = 1 # we want to change all the ratings to either 0 or 1 so if the user put a rating 3 or higher for the movie that indicates \r\n","# that the user did not like the movie"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kkL8NkkdlZj"},"source":["## Creating the architecture of the Neural Network\n"]},{"cell_type":"code","metadata":{"id":"z-Yd1mD8QUKz","executionInfo":{"status":"ok","timestamp":1609791385211,"user_tz":-240,"elapsed":792,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["class RBM(): # Here we making a class that will be the essense of building the architecture of the Restricted Boltzmann Machine\r\n","  \r\n","  \r\n","  def __init__(self, number_of_visible_nodes, number_of_hidden_nodes): # Initializng the class with two elements\r\n","\r\n","    self.weights = torch.randn(number_of_hidden_nodes, number_of_visible_nodes) # Initializng the weights of both the hidden and the visible nodes in a normal distribution \r\n","    # This will give as the weights as a matrix of dimension hidden_nodes x visible_nodes which all the possible connection between the nodes\r\n","\r\n","    self.bias_for_hidden_nodes = torch.randn(1, number_of_hidden_nodes) # Initializng the weights of both the hidden and the visible nodes in a normal distribution \r\n","    # This will give as the weights as a matrix of dimension hidden_nodes x visible_nodes which all the possible connection between the nodes\r\n","\r\n","    self.bias_for_visible_nodes = torch.randn(1, number_of_visible_nodes)# Initializng the weights of both the hidden and the visible nodes in a normal distribution \r\n","    # This will give as the weights as a matrix of dimension hidden_nodes x visible_nodes which all the possible connection between the nodes\r\n","\r\n","  \r\n","  def sample_hidden(self, x):\r\n","    weight_x_node = torch.mm(x, self.weights.t()) # Here we are multiplying the output of the node by the weight to determine then if the node is activated or not\r\n","    \r\n","    activation = weight_x_node +self.bias_for_hidden_nodes.expand_as(weight_x_node) # Here we are buidling the activation function of all the hidden nodes \r\n","    \r\n","    p_h_given_v = torch.sigmoid(activation) # Here we are simply getting the probability of activation of the hidden node  from the previous node which\r\n","    # is given to be visible using the sigmoid function and the value of the activation function done on all the hidden nodes\r\n","    \r\n","    return p_h_given_v, torch.bernoulli(p_h_given_v) # Here we are returning the probabilty calculated before and then the bernoulli distribution of the probabilities\r\n","    # either 0 or 1\r\n","\r\n","  \r\n","  def sample_visible(self, y):\r\n","    weight_y_node = torch.mm(y, self.weights) # Here we are multiplying the output of the node by the weight to determine then if the node is activated or not\r\n","    \r\n","    activation = weight_y_node +self.bias_for_visible_nodes.expand_as(weight_y_node) # Here we are buidling the activation function of all the visible nodes \r\n","    \r\n","    p_v_given_h = torch.sigmoid(activation) # Here we are simply getting the probability of activation of the visible node  from the previous node which\r\n","    # is given to be hidden using the sigmoid function and the value of the activation function done on all the visible nodes\r\n","    \r\n","    return p_v_given_h, torch.bernoulli(p_v_given_h) # Here we are returning the probabilty calculated before and then the bernoulli distribution of the probabilities\r\n","    # either 0 or 1\r\n","  \r\n","\r\n","  def train(self, input_vector, visible_nodes_after_k_iterations, prob_of_hidden_given_input_vector, prob_of_hidden_given_visible_nodes_after_k_iterations):\r\n","    self.weights += (torch.mm(input_vector.t(), prob_of_hidden_given_input_vector) - torch.mm(visible_nodes_after_k_iterations.t(), prob_of_hidden_given_visible_nodes_after_k_iterations)).t()\r\n","    \r\n","    self.bias_for_visible_nodes += torch.sum((input_vector - visible_nodes_after_k_iterations), 0)\r\n","\r\n","    self.bias_for_hidden_nodes += torch.sum((prob_of_hidden_given_input_vector - prob_of_hidden_given_visible_nodes_after_k_iterations), 0)\r\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"i5ZZi9vymzWp","executionInfo":{"status":"ok","timestamp":1609791387207,"user_tz":-240,"elapsed":1420,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}}},"source":["number_of_visible_nodes = len(training_set[0]) # The number of movies\r\n","number_of_hidden_nodes = 100 # The number of nodes we need our predictions to depend on\r\n","batch_size = 100 # The number of data we need to train our data on every epoch\r\n","\r\n","rbm = RBM(number_of_hidden_nodes = number_of_hidden_nodes, number_of_visible_nodes = number_of_visible_nodes) \r\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gy59alAdloL"},"source":["## Training the RBM\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qRNmi1FomeB","executionInfo":{"status":"ok","timestamp":1609791489425,"user_tz":-240,"elapsed":102589,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}},"outputId":"3d988581-df67-4bc6-c822-5dd0d1cbf39c"},"source":["number_of_epochs = 100\r\n","\r\n","for epoch in range(1, number_of_epochs + 1):\r\n","  train_loss = 0\r\n","  counter = 0.\r\n","  for id_user in range(0, nb_users- batch_size, batch_size):\r\n","    visible_node_after_k_iteration = training_set[id_user : id_user + batch_size]\r\n","    input_vector = training_set[id_user : id_user + batch_size]\r\n","    prob_of_hidden_given_input_vector, _ = rbm.sample_hidden(input_vector)\r\n","    for k in range(10):\r\n","      _, hidden_node_after_k_iteration = rbm.sample_hidden(visible_node_after_k_iteration)\r\n","      _, visible_node_after_k_iteration = rbm.sample_visible(hidden_node_after_k_iteration)\r\n","      visible_node_after_k_iteration[input_vector < 0] = input_vector[input_vector<0]\r\n","    prob_of_hidden_given_after_k_iteration, _ = rbm.sample_hidden(visible_node_after_k_iteration)\r\n","    rbm.train(input_vector, visible_node_after_k_iteration, prob_of_hidden_given_input_vector, prob_of_hidden_given_after_k_iteration)\r\n","    train_loss += torch.mean(torch.abs(input_vector[input_vector >= 0] - visible_node_after_k_iteration[input_vector>=0]))\r\n","    counter += 1\r\n","  print(\"Epoch: \"+str(epoch)+\" loss: \"+str(train_loss/counter))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch: 1 loss: tensor(0.3443)\n","Epoch: 2 loss: tensor(0.2507)\n","Epoch: 3 loss: tensor(0.2468)\n","Epoch: 4 loss: tensor(0.2505)\n","Epoch: 5 loss: tensor(0.2484)\n","Epoch: 6 loss: tensor(0.2479)\n","Epoch: 7 loss: tensor(0.2489)\n","Epoch: 8 loss: tensor(0.2480)\n","Epoch: 9 loss: tensor(0.2484)\n","Epoch: 10 loss: tensor(0.2457)\n","Epoch: 11 loss: tensor(0.2466)\n","Epoch: 12 loss: tensor(0.2478)\n","Epoch: 13 loss: tensor(0.2498)\n","Epoch: 14 loss: tensor(0.2462)\n","Epoch: 15 loss: tensor(0.2496)\n","Epoch: 16 loss: tensor(0.2459)\n","Epoch: 17 loss: tensor(0.2461)\n","Epoch: 18 loss: tensor(0.2457)\n","Epoch: 19 loss: tensor(0.2461)\n","Epoch: 20 loss: tensor(0.2469)\n","Epoch: 21 loss: tensor(0.2465)\n","Epoch: 22 loss: tensor(0.2459)\n","Epoch: 23 loss: tensor(0.2482)\n","Epoch: 24 loss: tensor(0.2443)\n","Epoch: 25 loss: tensor(0.2476)\n","Epoch: 26 loss: tensor(0.2472)\n","Epoch: 27 loss: tensor(0.2470)\n","Epoch: 28 loss: tensor(0.2450)\n","Epoch: 29 loss: tensor(0.2456)\n","Epoch: 30 loss: tensor(0.2462)\n","Epoch: 31 loss: tensor(0.2462)\n","Epoch: 32 loss: tensor(0.2467)\n","Epoch: 33 loss: tensor(0.2437)\n","Epoch: 34 loss: tensor(0.2471)\n","Epoch: 35 loss: tensor(0.2449)\n","Epoch: 36 loss: tensor(0.2470)\n","Epoch: 37 loss: tensor(0.2457)\n","Epoch: 38 loss: tensor(0.2477)\n","Epoch: 39 loss: tensor(0.2443)\n","Epoch: 40 loss: tensor(0.2483)\n","Epoch: 41 loss: tensor(0.2452)\n","Epoch: 42 loss: tensor(0.2463)\n","Epoch: 43 loss: tensor(0.2461)\n","Epoch: 44 loss: tensor(0.2441)\n","Epoch: 45 loss: tensor(0.2439)\n","Epoch: 46 loss: tensor(0.2445)\n","Epoch: 47 loss: tensor(0.2501)\n","Epoch: 48 loss: tensor(0.2483)\n","Epoch: 49 loss: tensor(0.2481)\n","Epoch: 50 loss: tensor(0.2481)\n","Epoch: 51 loss: tensor(0.2492)\n","Epoch: 52 loss: tensor(0.2471)\n","Epoch: 53 loss: tensor(0.2482)\n","Epoch: 54 loss: tensor(0.2463)\n","Epoch: 55 loss: tensor(0.2472)\n","Epoch: 56 loss: tensor(0.2473)\n","Epoch: 57 loss: tensor(0.2498)\n","Epoch: 58 loss: tensor(0.2449)\n","Epoch: 59 loss: tensor(0.2497)\n","Epoch: 60 loss: tensor(0.2486)\n","Epoch: 61 loss: tensor(0.2460)\n","Epoch: 62 loss: tensor(0.2476)\n","Epoch: 63 loss: tensor(0.2433)\n","Epoch: 64 loss: tensor(0.2486)\n","Epoch: 65 loss: tensor(0.2509)\n","Epoch: 66 loss: tensor(0.2445)\n","Epoch: 67 loss: tensor(0.2481)\n","Epoch: 68 loss: tensor(0.2475)\n","Epoch: 69 loss: tensor(0.2475)\n","Epoch: 70 loss: tensor(0.2468)\n","Epoch: 71 loss: tensor(0.2483)\n","Epoch: 72 loss: tensor(0.2478)\n","Epoch: 73 loss: tensor(0.2471)\n","Epoch: 74 loss: tensor(0.2474)\n","Epoch: 75 loss: tensor(0.2463)\n","Epoch: 76 loss: tensor(0.2429)\n","Epoch: 77 loss: tensor(0.2483)\n","Epoch: 78 loss: tensor(0.2467)\n","Epoch: 79 loss: tensor(0.2476)\n","Epoch: 80 loss: tensor(0.2454)\n","Epoch: 81 loss: tensor(0.2480)\n","Epoch: 82 loss: tensor(0.2467)\n","Epoch: 83 loss: tensor(0.2463)\n","Epoch: 84 loss: tensor(0.2456)\n","Epoch: 85 loss: tensor(0.2450)\n","Epoch: 86 loss: tensor(0.2499)\n","Epoch: 87 loss: tensor(0.2470)\n","Epoch: 88 loss: tensor(0.2450)\n","Epoch: 89 loss: tensor(0.2463)\n","Epoch: 90 loss: tensor(0.2490)\n","Epoch: 91 loss: tensor(0.2482)\n","Epoch: 92 loss: tensor(0.2451)\n","Epoch: 93 loss: tensor(0.2451)\n","Epoch: 94 loss: tensor(0.2495)\n","Epoch: 95 loss: tensor(0.2446)\n","Epoch: 96 loss: tensor(0.2476)\n","Epoch: 97 loss: tensor(0.2488)\n","Epoch: 98 loss: tensor(0.2462)\n","Epoch: 99 loss: tensor(0.2473)\n","Epoch: 100 loss: tensor(0.2453)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bak5uc8gd-gX"},"source":["## Testing the RBM\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7YVYCz9DNBo","executionInfo":{"status":"ok","timestamp":1609792338405,"user_tz":-240,"elapsed":819,"user":{"displayName":"mahmoud kharoof","photoUrl":"https://lh6.googleusercontent.com/-oJjhO6LTv0k/AAAAAAAAAAI/AAAAAAAAPxA/3js0ERV5mco/s64/photo.jpg","userId":"04175753233013773418"}},"outputId":"a643c1ed-3205-44a4-b0e5-b5d70962222e"},"source":["test_loss = 0\r\n","counter = 0.\r\n","\r\n","for id_user in range(nb_users):\r\n","   visible_node_after_1_iteration = training_set[id_user : id_user + 1]\r\n","   target_vector = test_set[id_user : id_user + 1]\r\n","   \r\n","   if len(target_vector[target_vector>=0]) > 0:\r\n","     _, hidden_node_after_1_iteration = rbm.sample_hidden(visible_node_after_1_iteration)\r\n","     _, visible_node_after_1_iteration = rbm.sample_visible(hidden_node_after_1_iteration)\r\n","     test_loss += torch.mean(torch.abs(target_vector[target_vector >= 0] - visible_node_after_1_iteration[target_vector>=0]))\r\n","     counter += 1.\r\n","\r\n","print(\"test loss: \"+str(test_loss/counter))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["test loss: tensor(0.2470)\n"],"name":"stdout"}]}]}