{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model 5 - ALBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8M_9KPB7-p7",
        "outputId": "513706e2-afba-4177-ab81-2d032ee606e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8smHaYE5NjS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "import time\n",
        "import heapq\n",
        "import tweepy # https://github.com/tweepy/tweepy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import configparser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Handler = 'elonmusk'\n",
        "path = 'drive/MyDrive/Colab Notebooks/COE494_Project'"
      ],
      "metadata": {
        "id": "38CtxbJR8E3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def authenticate(path = 'drive/MyDrive/Colab Notebooks/COE494_Project/'):\n",
        "  # read config\n",
        "  config = configparser.ConfigParser()\n",
        "  config.read(path + 'config.ini')\n",
        "\n",
        "  api_key = str(config['twitter']['api_key'])\n",
        "  api_key_secret = str(config['twitter']['api_key_secret'])\n",
        "\n",
        "  access_token = str(config['twitter']['access_token'])\n",
        "  access_token_secret = str(config['twitter']['access_token_secret'])\n",
        "\n",
        "  # authenticate\n",
        "  auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "  return tweepy.API(auth, wait_on_rate_limit = True)"
      ],
      "metadata": {
        "id": "Xa0hXyXe76I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = authenticate()"
      ],
      "metadata": {
        "id": "WhMor-Av-oVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweet text pre-processing\n",
        "def clean_tweet(tweet):\n",
        "    stopwords = [\"for\", \"on\", \"an\", \"a\", \"of\", \"and\", \"in\", \"the\", \"to\", \"from\"]\n",
        "    if type(tweet) == float:\n",
        "        return \"\"\n",
        "    temp = tweet.lower()\n",
        "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
        "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
        "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
        "    temp = re.sub(r'http\\S+', '', temp)\n",
        "    temp = re.sub('[()!?]', ' ', temp)\n",
        "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
        "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
        "    temp = temp.split()\n",
        "    temp = [w for w in temp if not w in stopwords]\n",
        "    temp = \" \".join(word for word in temp)\n",
        "    return temp"
      ],
      "metadata": {
        "id": "vqsKbwJLGq_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_tweets(handler):\n",
        "    # Twitter only allows access to a users most recent 3240 tweets with this method\n",
        "    print(f'Grabbing @{handler}\\'s Tweets')\n",
        "    #initialize a list to hold all the tweepy Tweets\n",
        "    all_tweets = []  \n",
        "    \n",
        "    # make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "    new_tweets = api.user_timeline(screen_name = handler, count = 200, include_rts = False, tweet_mode = 'extended')\n",
        "    \n",
        "    # save most recent tweets\n",
        "    all_tweets.extend(new_tweets)\n",
        "    \n",
        "    # save the id of the oldest tweet less one\n",
        "    oldest = all_tweets[-1].id - 1\n",
        "    \n",
        "    # keep grabbing tweets until there are no tweets left to grab\n",
        "    while len(new_tweets) > 0:        \n",
        "        # all subsiquent requests use the max_id param to prevent duplicates\n",
        "        new_tweets = api.user_timeline(screen_name = handler, count=200, max_id = oldest, include_rts = False, tweet_mode = 'extended')\n",
        "        # save most recent tweets\n",
        "        all_tweets.extend(new_tweets)        \n",
        "        # update the id of the oldest tweet less one\n",
        "        oldest = all_tweets[-1].id - 1\n",
        "        \n",
        "    print(f\"{len(all_tweets)} tweets downloaded...\")    \n",
        "    # transform the tweepy tweets into a 2D array that will populate the csv \n",
        "    out_tweets = [[tweet.id_str, tweet.created_at, tweet.full_text] for tweet in all_tweets]\n",
        "    df = pd.DataFrame (out_tweets, columns = [\"id\", \"time\", \"tweet\"])\n",
        "    df.to_csv(path + '/data/' + handler+'.csv')\n",
        "    return df"
      ],
      "metadata": {
        "id": "glkVV-wN5UF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tweets = get_all_tweets(Handler)\n",
        "tweets = pd.read_csv(path + \"/data/elonmusk.csv\")\n",
        "cleaned_tweets = pd.DataFrame([clean_tweet(tweet) for tweet in tweets.tweet], columns = ['tweet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiqa6IyV5UIQ",
        "outputId": "1b15f7b2-59cd-43e0-a33c-c60e239e531a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (2,4,9,14,16,17,19,22,24,25,26,31,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing null and empty rows \n",
        "cleaned_tweets.tweet.replace('', np.nan, inplace=True)\n",
        "cleaned_tweets.dropna(inplace = True)\n",
        "cleaned_tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_0rMIiyh5UKT",
        "outputId": "a649e5ba-614a-4c70-c345-aa78de0ae949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   tweet\n",
              "0      please ignore prior tweets as that was someone...\n",
              "1                                                so true\n",
              "2      if you ever wanted know real truth about moon ...\n",
              "3      walked around neighborhood recently rebuilt wi...\n",
              "4      it was xmas so we brought presents kids at orp...\n",
              "...                                                  ...\n",
              "34870  reminds me when i hex edited ultima v get out ...\n",
              "34871                                    yay switzerland\n",
              "34872  there is no way be touch with voters when you ...\n",
              "34874                     let s make roaring 20 s happen\n",
              "34875                 great work by tesla team worldwide\n",
              "\n",
              "[33624 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0c0705e-b029-43df-a6a8-7cf11b428d8e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>please ignore prior tweets as that was someone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>so true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>if you ever wanted know real truth about moon ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>walked around neighborhood recently rebuilt wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it was xmas so we brought presents kids at orp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34870</th>\n",
              "      <td>reminds me when i hex edited ultima v get out ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34871</th>\n",
              "      <td>yay switzerland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34872</th>\n",
              "      <td>there is no way be touch with voters when you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34874</th>\n",
              "      <td>let s make roaring 20 s happen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34875</th>\n",
              "      <td>great work by tesla team worldwide</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33624 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0c0705e-b029-43df-a6a8-7cf11b428d8e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f0c0705e-b029-43df-a6a8-7cf11b428d8e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f0c0705e-b029-43df-a6a8-7cf11b428d8e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_tweets_text = ' '.join(cleaned_tweets[\"tweet\"])"
      ],
      "metadata": {
        "id": "ojhGb1aDCecH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "id": "zk6rW5otHle_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install folium==0.2.1\n",
        "! pip3 install six == 1.15.0\n",
        "! pip3 install docutils == 0.10\n",
        "! pip3 install rsa == 3.1.2"
      ],
      "metadata": {
        "id": "RuiuK6pItBS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "metadata": {
        "id": "ooPXiYfbHlcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda(0)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "metadata": {
        "id": "NKeOLsgRHlXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48952448-dbc6-4baf-a372-147a43710c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:08<00:00, 46712956.84B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 2703022.64B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx"
      ],
      "metadata": {
        "id": "6QZWMlKCHlU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    #if rand_init:\n",
        "    #    for ii in range(max_len):\n",
        "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
        "    \n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
        "                        cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for all positions at a time step \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        for kk in range(max_len):\n",
        "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
        "            for jj in range(batch_size):\n",
        "                batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii, print_every) == 0:\n",
        "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
        "    \n",
        "    return untokenize_batch(batch)\n",
        "            \n",
        "def sequential_generation(seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
        "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
        "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    batch = batch.cuda() if cuda else batch\n",
        "    \n",
        "    for ii in range(max_len):\n",
        "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+ii] = idxs[jj]\n",
        "        \n",
        "        return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
        "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                               cuda=cuda, verbose=False)\n",
        "        \n",
        "        #batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\n",
        "        #batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\n",
        "        \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "mB0i8uyMHlSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "    with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "metadata": {
        "id": "RMcURFnzHlP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                          sample = sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                          cuda = True)\n",
        "    out_file = \"data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "metadata": {
        "id": "bzIdKj6GHlLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "9dd93b06-42c2-4e46-d8fe-b65a65fc9f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished batch 1 in 42.246s\n",
            "Finished batch 2 in 42.152s\n",
            "Finished batch 3 in 42.130s\n",
            "Finished batch 4 in 42.134s\n",
            "Finished batch 5 in 42.122s\n",
            "Finished batch 6 in 42.189s\n",
            "Finished batch 7 in 42.123s\n",
            "Finished batch 8 in 42.123s\n",
            "Finished batch 9 in 42.119s\n",
            "Finished batch 10 in 42.116s\n",
            "Finished batch 11 in 42.115s\n",
            "Finished batch 12 in 42.114s\n",
            "Finished batch 13 in 42.118s\n",
            "Finished batch 14 in 42.122s\n",
            "Finished batch 15 in 42.111s\n",
            "Finished batch 16 in 42.115s\n",
            "Finished batch 17 in 42.113s\n",
            "Finished batch 18 in 42.110s\n",
            "Finished batch 19 in 42.108s\n",
            "Finished batch 20 in 42.103s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-776125cadf25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                           cuda=True)\n\u001b[1;32m     19\u001b[0m     \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mwrite_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-8d3d4366cce8>\u001b[0m in \u001b[0;36mwrite_sents\u001b[0;34m(out_file, sents, should_detokenize)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_fh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshould_detokenize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/bert-base-uncased-len40-burnin250-topk100-temp1.000.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets.to_csv('tweets.txt', columns = ['tweet'], header = False, index = False)\n",
        "\n",
        "in_file = 'tweets.txt'\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "metadata": {
        "id": "kF2nkrPkHlJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    printer(bert_sents[i], should_detokenize=True)"
      ],
      "metadata": {
        "id": "XH8hA1NQHlFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd671a12-2bf9-4fa8-fe0d-a825a1feaa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c8e4a64129d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'printer' is not defined"
          ]
        }
      ]
    }
  ]
}